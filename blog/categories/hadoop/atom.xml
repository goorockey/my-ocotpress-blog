<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[categories: hadoop | Goorockey's Life]]></title>
  <link href="http://goorockey.github.com/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://goorockey.github.com/"/>
  <updated>2012-11-01T12:51:35+08:00</updated>
  <id>http://goorockey.github.com/</id>
  <author>
    <name><![CDATA[Goorockey]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[玩一下hadoop]]></title>
    <link href="http://goorockey.github.com/blog/2012/10/21/try-hadoop/"/>
    <updated>2012-10-21T23:47:00+08:00</updated>
    <id>http://goorockey.github.com/blog/2012/10/21/try-hadoop</id>
    <content type="html"><![CDATA[
<p>由于某种原因，今天玩了一下<a href="http://hadoop.apache.org/" title="Hadoop">Hadoop</a>。正确来说，我是玩<a href="http://code.google.com/p/hop/" title="HOP">HOP</a>，一个Hadoop的修改版本。</p>

<p><blockquote><p>The Hadoop Online Prototype (HOP) is a modified version of Hadoop MapReduce that allows data to be pipelined between tasks and between jobs. This can enable better cluster utilization and increased parallelism, and allows new functionality: online aggregation (approximate answers as a job runs), and stream processing (MapReduce jobs that run continuously, processing new data as it arrives).</p></blockquote></p>

<p>就是多了pipeline（流水线）的Hadoop。分布式流水线可以有效加快各jobs在各节点的同步运算。</p>

<!-- more -->

<h3 id="section">准备</h3>

<p>我是在linux上弄的，windows下用cygwin也行。</p>

<p>下载HOP压缩包后，看里面的docs就够了，同时src/example还有一些例子。</p>

<p>确保ssh,sshd,rsync,jdk都有了。同时要保证ssh localhost不要输入密码的认证步骤。具体docs/quickstart也有说，可以这样：</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ssh-keygen -t dsa -P ‘’ -f ~/.ssh/id_dsa
</span><span class='line'>$ cat ~/.ssh/id_dsa.pub » ~/.ssh/authorized_keys</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>然后是设置jdk的目录，修改conf/hadoop-env.sh中JAVA_HOME。一般为/usr/lib/jvm/下的某个java目录，我就直接写成/usr/lib/jvm/default-java了。</p>

<p>这时候执行bin/hadoop就会出现帮助信息了。</p>

<h3 id="section-1">跑例程</h3>

<p>Hadoop的文件系统叫<a href="http://hadoop.apache.org/docs/stable/hdfs_design.html" title="HDFS">HDFS</a>（Hadoop distribution filesystem)，是一个分布式文件系统。每份数据都会在多个节点有备份，以容错、修复。所有数据都要先放进HDFS才能Hadoop处理。</p>

<p>Hadoop的分布式体系中，有一个NameNode，是master的角色，负责主控各节点，有多个DataNode，是slave，负责真正存储数据。这些可以在conf/master和conf/slave设置。
同时还有一个JobTracker，负责调度jobs，默认就是NameNode这个主机一起充当NameNode，这个在conf/hadoop-site.xml设置。另外所有DataNode都是TaskTracker，负责执行jobs。具体更多对conf/hadoop-site.xml的配置参看docs/cluster_setup.html</p>

<p>执行bin/hadoop namenode -format，会创造一个namenode。文件都已某种格式放在/tmp/hadoop-“hostname”那里。</p>

<p>执行bin/start-all.sh会启动hadoop，默认通过http://localhost:50070/可以访问NameNode，http://localhost:50030/可以访问JobTracker。</p>

<p>现在执行一个例子:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ mkdir input
</span><span class='line'>$ cp conf/*.xml input/&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>$ bin/hadoop fs -put intput input   # 把当前文件系统input目录复制为HDFS的input
</span><span class='line'>$ bin/hadoop jar hadoop-*-examples.jar grep input output ‘dfs[a-z.]+’  # 执行所有example.jar，后面的是参数&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;h1 id="section-2">一段时间后，执行完毕&lt;/h1>
</span><span class='line'>&lt;p>$ bin/hadoop fs -get output output # 把HDFS中的output目录复制为当前文件系统的ouput
</span><span class='line'>$ cat output/* # 打印结果&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;h1 id="hdfs">或者直接对HDFS操作&lt;/h1>
</span><span class='line'>&lt;p>$ bin/hadoop fs -ls output
</span><span class='line'>$ bin/hadoop fs -cat output/*</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<h3 id="wordcount">WordCount例子</h3>

<p><a href="http://hadoop.apache.org/docs/stable/mapred_tutorial.html" title="WordCount">WordCount</a>是hadoop中的另一个例子</p>

<p>Hadoop是通过<a href="http://wiki.apache.org/hadoop/HadoopMapReduce" title="MapReduce">MapReduce</a>机制来处理大数据的。Map阶段分割输入的数据，并整合成&lt;key,value&gt;的对应关系。每对&lt;key,value&gt;对送到Combiner做每个key的整合，当整合出一定数量的&lt;key,value&gt;后，&lt;key,value&gt;会送到Reducer做处理输出最终的&lt;key,value&gt;。</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>(input) &lt;k1, v1&gt; -&gt; map -&gt; &lt;k2, v2&gt; -&gt; combine -&gt; &lt;k2, v2&gt; -&gt; reduce -&gt; &lt;k3, v3&gt; (output)</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>按照<a href="http://hadoop.apache.org/docs/stable/mapred_tutorial.html" title="WordCount">WordCount</a>中的代码编辑WordCount.java，然后编译打包生成wordcount.jar:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ mkdir wordcount_classes
</span><span class='line'>$ javac -classpath hadoop-hop-0.2-core.jar -d wordcount_classes WordCount.java
</span><span class='line'>$ jar -cvf wordcount.jar -C wordcount_classes/ .</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>然后自行构造一些要统计的文件，放在input目录下。这时候注意，在执行了上一次例子后，如果想把输入文件还是放在HDFS的input下，要先清空原来的文件:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ bin/hadoop fs -rmr input/
</span><span class='line'>$ bin/hadoop fs -rmr output/&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>$ bin/hadoop fs -put input input # 把输入文件目录input重新放到HDFS中&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>$ bin/hadoop jar wordcount.jar org.myorg.WordCount input output  # 执行wordcount.jar&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;h1 id="section-3">执行一段时间后完毕&lt;/h1>
</span><span class='line'>
</span><span class='line'>&lt;p>$ bin/hadoop fs -cat output/*  # 打印结果</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<h3 id="section-4">结语</h3>

<p>尝试了一下Hadoop，还有更多有待研究</p>
]]></content>
  </entry>
  
</feed>
